{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estudo parametrico de um algoritmo imunológico em um problema de classificação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separar em 10 pastas de tamanho igual (15)\n",
    "\n",
    "# 9 para treino e 1 para teste\n",
    "\n",
    "# 10 execuções\n",
    "\n",
    "# Cada execução testo numa nova pasta (decremental)\n",
    "\n",
    "# Média e desvio padrão dos 10 testes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install numpy\n",
    "# !pip install pandas\n",
    "# !pip install sklearn\n",
    "# !pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import random\n",
    "import clonalg\n",
    "import pandas as pd\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Anticorpos treinados com exemplos de iris virginica [label 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('dataset/iris.csv', header=None)\n",
    "\n",
    "df[4] = pd.Categorical(df[4]).codes\n",
    "labels = df[[4]].copy()\n",
    "\n",
    "df = df.drop(columns=[4])\n",
    "\n",
    "#Standard Scaler\n",
    "scaler = preprocessing.StandardScaler()\n",
    "train = pd.DataFrame(scaler.fit_transform(df.values), columns=df.columns, index=df.index)\n",
    "\n",
    "#Train dataset\n",
    "train = train.join(labels)\n",
    "# O valor de train[4] define qual label será utilizada para o treino da população\n",
    "train = train.loc[train[4] == 0]\n",
    "train = train.drop(columns=[4])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Métodos de normalização além do StandartScaler deverão ser testados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "feature_num = len(train.axes[1])\n",
    "feature_max = train.max()\n",
    "feature_max  = feature_max.max()\n",
    "feature_min = train.min()\n",
    "feature_min  = feature_min.min()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Definição de hiperparâmetros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "population_size = 400\n",
    "selection_size = 50\n",
    "memory_set_percentage = 50\n",
    "\n",
    "clone_rate = 40 ## Clone rate - 1 (Step 5 +-)\n",
    "mutation_rate = 0.3\n",
    "stop_codition = 100\n",
    "\n",
    "\n",
    "d = 15 ## Individuos aleatorios - 2 (Step 2.5 +-)\n",
    "sigma1 = 0.9\n",
    "sigma2 = 0.3"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Conversão de dataframe para numpy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "train = train.to_numpy()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "![](resources/V7zasui.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # 1. For each antigen do\n",
    "    # 1.1 Determine its affinity to network cells\n",
    "    # 1.2 Select the n highest affinity network cells\n",
    "    # 1.3 Generate Nc clones from these n cells. The higher the affinity, the larger Nc;\n",
    "    # 1.4 Apply hypermutation to the generated clones, with variability inversely proportional to the progenitor fitness \n",
    "    # 1.5 Determine the affinity among the antigen and all clones\n",
    "    # 1.6 Keep only m% of the highest affinity mutated clones into the clone population\n",
    "    # 1.7 Eliminate all clones but one whose affinity with the antigen is inferior to a predefined threshold sigma2 (apoptosis)\n",
    "    # 1.8 Determine the affinity among all the mutated clonesand eliminate those whose affinity with each other is above a pre-defined threshold sigma1 (supression)\n",
    "    # 1.9 Insert the remaining clones into the population\n",
    "# 2. Determine the simillarity among all the antibodies and eliminate those with similarity above a threshold sigma1 (supression)\n",
    "# 3. Introduce a d% of new randomly generated cells (random insertion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "stop = 0\n",
    "population = clonalg.create_random_cells(population_size, feature_num, feature_min, feature_max)\n",
    "\n",
    "while stop != stop_codition:\n",
    "    # 1. For each antigen do\n",
    "    for antigen in train:\n",
    "        # 1.1 Determine its affinity to network cells\n",
    "        population_affinity = [(cell, clonalg.affinity(cell, antigen)) for cell in population]\n",
    "        # 1.2 Select the n highest affinity network cells\n",
    "        population_affinity = sorted(population_affinity, key=lambda x: abs(x[1]))\n",
    "        best_affinity = population_affinity[:selection_size]\n",
    "        # 1.3 Generate Nc clones from these n cells. The higher the affinity, the larger Nc;\n",
    "        clone_population = []\n",
    "        for cell in best_affinity:\n",
    "            cell_clones = clonalg.clone(cell, clone_rate)\n",
    "            clone_population += cell_clones\n",
    "        # 1.4 Apply hypermutation to the generated clones, with variability inversely proportional to the progenitor fitness \n",
    "        # 1.5 Determine the affinity among the antigen and all clones\n",
    "        mutaded_clone_population = []\n",
    "        for cell in clone_population:\n",
    "            mutated_clone = clonalg.hypermutate_variability(cell, mutation_rate, antigen)\n",
    "            mutaded_clone_population.append(mutated_clone)\n",
    "        # 1.6 Keep only m% of the highest affinity mutated clones into the clone population\n",
    "        mutaded_clone_population.sort(key=lambda x: x[1])\n",
    "        pop_size = round(len(clone_population)/100)*memory_set_percentage\n",
    "\n",
    "        mutaded_clone_population = mutaded_clone_population[:pop_size]\n",
    "\n",
    "        # 1.7 Eliminate all clones but one whose affinity with the antigen is inferior to a predefined threshold sigma2 (apoptosis)\n",
    "        filtered_clone_population = list(filter(lambda x: x[1] < sigma2, mutaded_clone_population))\n",
    "\n",
    "        # 1.8 Determine the affinity among all the mutated clonesand eliminate those whose affinity with each other is above a pre-defined threshold sigma1 (supression)\n",
    "        remaining_clone_population = clonalg.remove_similar_clones(filtered_clone_population, sigma1)\n",
    "\n",
    "        # 1.9 Insert the remaining clones into the populatuon\n",
    "        # Remova o atributo de afinidade das células em remaining_clone_population\n",
    "        remaining_clone_population_no_affinity = [(cell[0],) for cell in remaining_clone_population]\n",
    "        # Adicione remaining_clone_population_no_affinity à população\n",
    "        population = population + remaining_clone_population_no_affinity\n",
    "\n",
    "    # 2.0 Determine the simillarity among all the antibodies and eliminate those with similarity above a threshold sigma1 (supression)\n",
    "    population = clonalg.suppress_similar_cells(population, sigma1)\n",
    "\n",
    "    # 3 Introduce a d% of new randomly generated cells (random insertion)\n",
    "    new_cells = clonalg.create_random_cells(int(population_size * (d / 100)), feature_num, feature_min, feature_max)\n",
    "    population += new_cells\n",
    "    print(\"População: \", len(population), \"     Iteração: \", stop)\n",
    "    stop += 1\n",
    "\n",
    "## Solução temporária para nested arrays nos indivíduos da população.\n",
    "\n",
    "for i, cell in enumerate(population):\n",
    "    # Verifica se a célula tem mais de uma dimensão\n",
    "    if len(cell[0].shape) > 1:\n",
    "        # Aplica numpy.ravel() para simplificar as dimensões\n",
    "        flattened_cell = np.ravel(cell[0])\n",
    "        population[i] = (flattened_cell,)\n",
    "    else:\n",
    "        population[i] = cell\n",
    "\n",
    "    if isinstance(cell, tuple):\n",
    "        array_cell = np.array(cell)\n",
    "        flattened_cell = np.ravel(array_cell)\n",
    "        population[i] = (flattened_cell)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.shape(population)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, cell in enumerate(population):\n",
    "    # Verifica se a célula tem mais de uma dimensão\n",
    "    if len(cell[0].shape) > 1:\n",
    "        # Aplica numpy.ravel() para simplificar as dimensões\n",
    "        flattened_cell = np.ravel(cell[0])\n",
    "        population[i] = (flattened_cell,)\n",
    "    else:\n",
    "        population[i] = cell\n",
    "        \n",
    "    if isinstance(cell, tuple):\n",
    "        array_cell = np.array(cell)\n",
    "        flattened_cell = np.ravel(array_cell)\n",
    "        population[i] = (flattened_cell)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "antibodies = pd.DataFrame(population)\n",
    "antibodies = pd.DataFrame(scaler.inverse_transform(antibodies), columns=antibodies.columns)\n",
    "antibodies_array = antibodies.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "antibodies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Extract the feature vectors from the population\n",
    "feature_vectors = np.array(list(antibodies_array))\n",
    "\n",
    "\n",
    "# Apply PCA to reduce the dimensionality to 3\n",
    "pca = PCA(n_components=3)\n",
    "reduced_population = pca.fit_transform(feature_vectors)\n",
    "\n",
    "# Now, you can plot the reduced population using a 3D scatter plot\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "x = reduced_population[:, 0]\n",
    "y = reduced_population[:, 1]\n",
    "z = reduced_population[:, 2]\n",
    "\n",
    "ax.scatter(x, y, z)\n",
    "\n",
    "ax.set_xlabel('Principal Component 1')\n",
    "ax.set_ylabel('Principal Component 2')\n",
    "ax.set_zlabel('Principal Component 3')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "feature_vectors = np.array(list(antibodies_array))\n",
    "\n",
    "x = feature_vectors[:, 0]\n",
    "y = feature_vectors[:, 1]\n",
    "z = feature_vectors[:, 2]\n",
    "c = feature_vectors[:, 3]\n",
    "\n",
    "img = ax.scatter(x, y, z, c=c, cmap=plt.hot())\n",
    "fig.colorbar(img)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('dataset/iris.csv', header=None)\n",
    "\n",
    "df[4] = pd.Categorical(df[4]).codes\n",
    "labels = df[[4]].copy()\n",
    "\n",
    "df = df.drop(columns=[4])\n",
    "\n",
    "#Standard Scaler\n",
    "scaler = preprocessing.StandardScaler()\n",
    "train = pd.DataFrame(df)\n",
    "\n",
    "#Train dataset\n",
    "train = train.join(labels)\n",
    "train = train.loc[train[4] == 2]\n",
    "train = train.drop(columns=[4])\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "train_array = train.values\n",
    "\n",
    "feature_vectors = np.array(list(train_array))\n",
    "\n",
    "x = feature_vectors[:, 0]\n",
    "y = feature_vectors[:, 1]\n",
    "z = feature_vectors[:, 2]\n",
    "c = feature_vectors[:, 3]\n",
    "\n",
    "img = ax.scatter(x, y, z, c=c, cmap=plt.hot())\n",
    "fig.colorbar(img)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('dataset/iris.csv', header=None)\n",
    "\n",
    "df[4] = pd.Categorical(df[4]).codes\n",
    "labels = df[[4]].copy()\n",
    "\n",
    "df = df.drop(columns=[4])\n",
    "\n",
    "#Standard Scaler\n",
    "scaler = preprocessing.StandardScaler()\n",
    "train = pd.DataFrame(df)\n",
    "\n",
    "#Train dataset\n",
    "train = train.join(labels)\n",
    "train = train.loc[train[4] == 1]\n",
    "train = train.drop(columns=[4])\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "train_array = train.values\n",
    "\n",
    "feature_vectors = np.array(list(train_array))\n",
    "\n",
    "x = feature_vectors[:, 0]\n",
    "y = feature_vectors[:, 1]\n",
    "z = feature_vectors[:, 2]\n",
    "c = feature_vectors[:, 3]\n",
    "\n",
    "img = ax.scatter(x, y, z, c=c, cmap=plt.hot())\n",
    "fig.colorbar(img)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('dataset/iris.csv', header=None)\n",
    "\n",
    "df[4] = pd.Categorical(df[4]).codes\n",
    "labels = df[[4]].copy()\n",
    "\n",
    "df = df.drop(columns=[4])\n",
    "\n",
    "#Standard Scaler\n",
    "scaler = preprocessing.StandardScaler()\n",
    "train = pd.DataFrame(df)\n",
    "\n",
    "#Train dataset\n",
    "train = train.join(labels)\n",
    "train = train.loc[train[4] == 0]\n",
    "train = train.drop(columns=[4])\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "train_array = train.values\n",
    "\n",
    "feature_vectors = np.array(list(train_array))\n",
    "\n",
    "x = feature_vectors[:, 0]\n",
    "y = feature_vectors[:, 1]\n",
    "z = feature_vectors[:, 2]\n",
    "c = feature_vectors[:, 3]\n",
    "\n",
    "img = ax.scatter(x, y, z, c=c, cmap=plt.hot())\n",
    "fig.colorbar(img)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn import preprocessing\n",
    "from train_loop import train_ais_classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('dataset/iris.csv', header=None)\n",
    "\n",
    "df[4] = pd.Categorical(df[4]).codes\n",
    "labels = df[[4]].copy()\n",
    "\n",
    "df = df.drop(columns=[4])\n",
    "\n",
    "# Standard Scaler\n",
    "scaler = preprocessing.StandardScaler()\n",
    "train = pd.DataFrame(scaler.fit_transform(df.values), columns=df.columns, index=df.index)\n",
    "train = train.join(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_num = train.shape[1] - 1  # Menos a coluna de labels\n",
    "feature_min = train.iloc[:, :-1].min().min()  # Menos a coluna de labels\n",
    "feature_max = train.iloc[:, :-1].max().max()  # Menos a coluna de labels\n",
    "\n",
    "# Treine o classificador AIS para cada classe (0, 1, 2) e armazene os resultados em uma lista\n",
    "trained_populations = []\n",
    "for class_label in range(3):\n",
    "    train_class = train.loc[train[4] == class_label]\n",
    "    train_class = train_class.drop(columns=[4])\n",
    "    train_class = train_class.to_numpy()\n",
    "    population = train_ais_classifier(train_class, feature_num, feature_min, feature_max)\n",
    "    trained_populations.append(population)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import clonalg\n",
    "from train_loop import multiclass_performance_measure\n",
    "\n",
    "df = pd.read_csv('dataset/iris.csv', header=None)\n",
    "\n",
    "df[4] = pd.Categorical(df[4]).codes\n",
    "labels = df[[4]].copy()\n",
    "\n",
    "df = df.drop(columns=[4])\n",
    "\n",
    "# Standard Scaler\n",
    "scaler = preprocessing.StandardScaler()\n",
    "train = pd.DataFrame(scaler.fit_transform(df.values), columns=df.columns, index=df.index)\n",
    "train = train.join(labels)\n",
    "# Dividir o conjunto de dados em treino e teste\n",
    "train_data, test_data = train_test_split(train, test_size=0.3, random_state=42)\n",
    "\n",
    "test_data_array = test_data.values\n",
    "performance = multiclass_performance_measure(trained_populations, test_data_array)\n",
    "print(f\"Desempenho do classificador AIS: {performance * 100:.2f}%\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multithread"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from train_loop import train_clonalg_parallel\n",
    "from train_loop import multiclass_performance_measure_v2\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "from sklearn import preprocessing\n",
    "\n",
    "df = pd.read_csv('dataset/iris.csv', header=None)\n",
    "\n",
    "df[4] = pd.Categorical(df[4]).codes\n",
    "labels = df[[4]].copy()\n",
    "\n",
    "df = df.drop(columns=[4])\n",
    "\n",
    "# Standard Scaler\n",
    "scaler = preprocessing.MinMaxScaler()\n",
    "train = pd.DataFrame(scaler.fit_transform(df.values), columns=df.columns, index=df.index)\n",
    "train = train.join(labels)\n",
    "# Dividir o conjunto de dados em treino e teste\n",
    "train_data, test_data = train_test_split(train, test_size=0.3, random_state=82)\n",
    "\n",
    "params = {\n",
    "    'population_size': 200,\n",
    "    'selection_size': 30,\n",
    "    'memory_set_percentage': 15,\n",
    "    'clone_rate': 25,\n",
    "    'mutation_rate': 0.5,\n",
    "    'stop_condition': 200,\n",
    "    'd': 60,\n",
    "    'sigma1': 0.9,\n",
    "    'sigma2': 0.6,\n",
    "    'feature_num': train_data.shape[1] - 1,\n",
    "    'feature_min': train_data.min().min(),\n",
    "    'feature_max': train_data.max().max(),\n",
    "}\n",
    "\n",
    "trained_populations = train_clonalg_parallel(train_data, params, [0, 1, 2])\n",
    "\n",
    "# Avaliar o desempenho do classificador AIS\n",
    "test_data_array = test_data.values\n",
    "performance, ltrue, lpredict = multiclass_performance_measure_v2(trained_populations, test_data_array)\n",
    "print(f\"Desempenho do classificador AIS: {performance * 100:.2f}%\")\n",
    "print(lpredict)\n",
    "print(ltrue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import clonalg\n",
    "\n",
    "example = [0.19444444, 0.625     , 0.10169492, 0.20833333]\n",
    "highest1 = []\n",
    "highest2 = []\n",
    "highest3 = []\n",
    "\n",
    "for cell in trained_populations[0]:\n",
    "    highest1.append(clonalg.affinity(example, cell))\n",
    "print(np.sort(highest1))\n",
    "\n",
    "for cell in trained_populations[1]:\n",
    "    highest2.append(clonalg.affinity(example, cell))\n",
    "print(np.sort(highest2))\n",
    "\n",
    "for cell in trained_populations[2]:\n",
    "    highest3.append(clonalg.affinity(example, cell))\n",
    "print(np.sort(highest3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range (3):\n",
    "    trained_populations[i] = scaler.inverse_transform(trained_populations[i])\n",
    "\n",
    "test_data_test = pd.DataFrame(test_data_array)\n",
    "labels = test_data_test[[4]].copy()\n",
    "test_data_test= test_data_test.drop(columns=[4])\n",
    "test_data_test = pd.DataFrame(scaler.inverse_transform(test_data_test.values), columns=test_data_test.columns, index=test_data_test.index)\n",
    "test_data_test = test_data_test.join(labels)\n",
    "test_data_array= test_data_test.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "performance, ltrue, lpredict = multiclass_performance_measure_v2(trained_populations, test_data_array)\n",
    "print(f\"Desempenho do classificador AIS: {performance * 100:.2f}%\")\n",
    "print(lpredict)\n",
    "print(ltrue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "from train_loop import multiclass_performance_measure_v2\n",
    "from train_loop import train_clonalg_parallel\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import warnings\n",
    "from sklearn.exceptions import UndefinedMetricWarning\n",
    "warnings.filterwarnings(\"ignore\", category=UndefinedMetricWarning)\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "from sklearn import preprocessing\n",
    "\n",
    "df = pd.read_csv('dataset/iris.csv', header=None)\n",
    "\n",
    "df[4] = pd.Categorical(df[4]).codes\n",
    "labels = df[[4]].copy()\n",
    "\n",
    "df = df.drop(columns=[4])\n",
    "\n",
    "# Standard Scaler\n",
    "scaler = preprocessing.MinMaxScaler()\n",
    "train = pd.DataFrame(scaler.fit_transform(df.values), columns=df.columns, index=df.index)\n",
    "train = train.join(labels)\n",
    "\n",
    "# Crie um objeto KFold\n",
    "kfold = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "# Armazenar as métricas de desempenho de cada iteração\n",
    "performance_scores = []\n",
    "# Armazenar todas as verdadeiras e previstas etiquetas através das divisões\n",
    "all_true_labels = []\n",
    "all_predicted_labels = []\n",
    "\n",
    "train_data, test_data = train_test_split(train, test_size=0.3, random_state=42)\n",
    "\n",
    "params = {\n",
    "    'population_size': 100,\n",
    "    'selection_size': 25,\n",
    "    'memory_set_percentage': 15,\n",
    "    'clone_rate': 10,\n",
    "    'mutation_rate': 0.3,\n",
    "    'stop_condition': 5,\n",
    "    'd': 15,\n",
    "    'sigma1': 0.8,\n",
    "    'sigma2': 0.8,\n",
    "    'feature_num': train_data.shape[1] - 1,\n",
    "    'feature_min': train_data.min().min(),\n",
    "    'feature_max': train_data.max().max(),\n",
    "}\n",
    "\n",
    "# Loop através das divisões de treinamento e teste\n",
    "for i, (train_index, test_index) in enumerate(kfold.split(train), start=1):\n",
    "    # Dividir o conjunto de dados em treinamento e teste com base nos índices\n",
    "    train_data = train.iloc[train_index]\n",
    "    test_data = train.iloc[test_index]\n",
    "\n",
    "    # Treinar o classificador AIS\n",
    "    trained_populations = train_clonalg_parallel(train_data, params, [0, 1, 2])\n",
    "\n",
    "    # Avaliar o desempenho do classificador AIS\n",
    "    test_data_array = test_data.values\n",
    "    accuracy, true_labels, predicted_labels = multiclass_performance_measure_v2(trained_populations, test_data_array)\n",
    "    performance_scores.append(accuracy)\n",
    "\n",
    "    # Estender all_true_labels e all_predicted_labels com os rótulos verdadeiros e previstos da iteração atual\n",
    "    all_true_labels.extend(true_labels)\n",
    "    all_predicted_labels.extend(predicted_labels)\n",
    "\n",
    "    # Imprimir o relatório de classificação para a iteração atual\n",
    "    print(\"\\n//////////////////////////////////////////////\\n\")\n",
    "    print(f\"Relatório de classificação para a pasta {i}:\")\n",
    "    print(classification_report(true_labels, predicted_labels))\n",
    "    print(\"\\n//////////////////////////////////////////////\\n\")\n",
    "\n",
    "# Calcular a precisão média\n",
    "print(\"\\n//////////////////////////////////////////////\\n\")\n",
    "mean_accuracy = np.mean(performance_scores)\n",
    "print(f\"Desempenho médio do classificador AIS: {mean_accuracy * 100:.2f}%\")\n",
    "print(\"\\n//////////////////////////////////////////////\\n\")\n",
    "\n",
    "# Calcular e exibir a matriz de confusão combinada\n",
    "print(\"\\n//////////////////////////////////////////////\\n\")\n",
    "conf_matrix = confusion_matrix(all_true_labels, all_predicted_labels)\n",
    "print(\"Matriz de confusão:\")\n",
    "print(conf_matrix)\n",
    "print(\"\\n//////////////////////////////////////////////\\n\")\n",
    "\n",
    "# Calcular e exibir o relatório de classificação combinado\n",
    "print(\"\\n//////////////////////////////////////////////\\n\")\n",
    "class_report = classification_report(all_true_labels, all_predicted_labels)\n",
    "print(\"Relatório de classificação:\")\n",
    "print(class_report)\n",
    "print(\"\\n//////////////////////////////////////////////\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from train_loop import train_clonalg_parallel\n",
    "from train_loop import multiclass_performance_measure_v2\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "from sklearn import preprocessing\n",
    "\n",
    "df = pd.read_csv('dataset/wine.csv', header=None)\n",
    "\n",
    "labels = df[[0]].copy()\n",
    "\n",
    "df = df.drop(columns=[0])\n",
    "\n",
    "# Standard Scaler\n",
    "scaler = preprocessing.MinMaxScaler()\n",
    "train = pd.DataFrame(scaler.fit_transform(df.values), columns=df.columns, index=df.index)\n",
    "train = train.join(labels)\n",
    "# Dividir o conjunto de dados em treino e teste\n",
    "train_data, test_data = train_test_split(train, test_size=0.3, random_state=52)\n",
    "\n",
    "params = {\n",
    "    'population_size': 40,\n",
    "    'selection_size': 15,\n",
    "    'memory_set_percentage': 15,\n",
    "    'clone_rate': 25,\n",
    "    'mutation_rate': 0.3,\n",
    "    'stop_condition': 5,\n",
    "    'd': 15,\n",
    "    'sigma1': 0.95,\n",
    "    'sigma2': 0.5,\n",
    "    'feature_num': train_data.shape[1] - 1,\n",
    "    'feature_min': train_data.min().min(),\n",
    "    'feature_max': train_data.max().max(),\n",
    "}\n",
    "\n",
    "trained_populations = train_clonalg_parallel(train_data, params, [1, 2, 3])\n",
    "\n",
    "# Avaliar o desempenho do classificador AIS\n",
    "test_data_array = test_data.values\n",
    "performance, ltrue, lpredict = multiclass_performance_measure_v2(trained_populations, test_data_array)\n",
    "print(f\"Desempenho do classificador AIS: {performance * 100:.2f}%\")\n",
    "print(lpredict)\n",
    "print(ltrue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from train_loop import multiclass_performance_measure_v2\n",
    "\n",
    "test_data_array = test_data.values\n",
    "performance, ltrue, lpredict = multiclass_performance_measure_v2(trained_populations, test_data_array)\n",
    "print(f\"Desempenho do classificador AIS: {performance * 100:.2f}%\")\n",
    "print(lpredict)\n",
    "print(ltrue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "from train_loop import train_clonalg_parallel\n",
    "from train_loop import multiclass_performance_measure_v2\n",
    "\n",
    "# Criar um conjunto de dados com 3 clusters bem separados\n",
    "X, y = make_blobs(n_samples=300, centers=3, random_state=42)\n",
    "\n",
    "# Converter os arrays em um DataFrame\n",
    "df = pd.DataFrame(np.column_stack((X, y)))\n",
    "\n",
    "# Dividir o conjunto de dados em treino e teste\n",
    "train_data, test_data = train_test_split(df, test_size=0.3)\n",
    "\n",
    "params = {\n",
    "    'population_size': 200,\n",
    "    'selection_size': 30,\n",
    "    'memory_set_percentage': 20,\n",
    "    'clone_rate': 25,\n",
    "    'mutation_rate': 0.3,\n",
    "    'stop_condition': 200,\n",
    "    'd': 5,\n",
    "    'sigma1': 0.95,\n",
    "    'sigma2': 0.6,\n",
    "    'feature_num': train_data.shape[1] - 1,\n",
    "    'feature_min': train_data.min().min(),\n",
    "    'feature_max': train_data.max().max(),\n",
    "}\n",
    "\n",
    "trained_populations = train_clonalg_parallel(train_data, params, [0, 1, 2])\n",
    "\n",
    "# Avaliar o desempenho do classificador AIS\n",
    "test_data_array = test_data.values\n",
    "performance, ltrue, lpredict = multiclass_performance_measure_v2(trained_populations, test_data_array)\n",
    "print(f\"Desempenho do classificador AIS: {performance * 100:.2f}%\")\n",
    "print(lpredict)\n",
    "print(ltrue)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_antibody_populations(trained_populations, colors=['r', 'g', 'b'], markers=['o', '^', 's']):\n",
    "    for i, population in enumerate(trained_populations):\n",
    "        antibody_coordinates = [cell for cell in population]\n",
    "        x_coordinates = [coord[0] for coord in antibody_coordinates]\n",
    "        y_coordinates = [coord[1] for coord in antibody_coordinates]\n",
    "        plt.scatter(x_coordinates, y_coordinates, color=colors[i], marker=markers[i], label=f\"Population {i}\")\n",
    "\n",
    "    plt.xlabel('Feature 1')\n",
    "    plt.ylabel('Feature 2')\n",
    "    plt.title('Antibody Populations')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "plot_antibody_populations(trained_populations)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(trained_populations[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import clonalg\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "\n",
    "# Criar um conjunto de dados com 3 clusters bem separados\n",
    "X, y = make_blobs(n_samples=300, centers=3, random_state=42)\n",
    "\n",
    "# Converter os arrays em um DataFrame\n",
    "df = pd.DataFrame(np.column_stack((X, y)))\n",
    "\n",
    "# Dividir o conjunto de dados em treino e teste\n",
    "train_data, test_data = train_test_split(df, test_size=0.3)\n",
    "\n",
    "train_data = train_data.to_numpy()\n",
    "\n",
    "labels = df[[2]].copy()\n",
    "\n",
    "df = df.drop(columns=[2])\n",
    "\n",
    "#Standard Scaler\n",
    "scaler = preprocessing.MinMaxScaler()\n",
    "train = pd.DataFrame(scaler.fit_transform(df.values), columns=df.columns, index=df.index)\n",
    "\n",
    "#Train dataset\n",
    "train = train.join(labels)\n",
    "# O valor de train[4] define qual label será utilizada para o treino da população\n",
    "train = train.loc[train[2] == 2]\n",
    "train = train.drop(columns=[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "População:  52      Iteração:  0\n",
      "População:  61      Iteração:  1\n",
      "População:  69      Iteração:  2\n",
      "População:  72      Iteração:  3\n",
      "População:  79      Iteração:  4\n",
      "População:  76      Iteração:  5\n",
      "População:  83      Iteração:  6\n",
      "População:  81      Iteração:  7\n",
      "População:  86      Iteração:  8\n",
      "População:  85      Iteração:  9\n",
      "População:  87      Iteração:  10\n",
      "População:  87      Iteração:  11\n",
      "População:  85      Iteração:  12\n",
      "População:  86      Iteração:  13\n",
      "População:  85      Iteração:  14\n",
      "População:  81      Iteração:  15\n",
      "População:  83      Iteração:  16\n",
      "População:  85      Iteração:  17\n",
      "População:  83      Iteração:  18\n",
      "População:  81      Iteração:  19\n",
      "População:  83      Iteração:  20\n",
      "População:  84      Iteração:  21\n",
      "População:  84      Iteração:  22\n",
      "População:  83      Iteração:  23\n",
      "População:  84      Iteração:  24\n",
      "População:  86      Iteração:  25\n",
      "População:  83      Iteração:  26\n",
      "População:  83      Iteração:  27\n",
      "População:  83      Iteração:  28\n",
      "População:  83      Iteração:  29\n",
      "População:  85      Iteração:  30\n",
      "População:  86      Iteração:  31\n",
      "População:  84      Iteração:  32\n",
      "População:  82      Iteração:  33\n",
      "População:  85      Iteração:  34\n",
      "População:  84      Iteração:  35\n",
      "População:  83      Iteração:  36\n",
      "População:  83      Iteração:  37\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "setting an array element with a sequence. The requested array would exceed the maximum number of dimension of 32.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[31], line 54\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[39mwhile\u001b[39;00m stop \u001b[39m!=\u001b[39m stop_condition:\n\u001b[1;32m     51\u001b[0m     \u001b[39m# 1. For each antigen do\u001b[39;00m\n\u001b[1;32m     52\u001b[0m     \u001b[39mfor\u001b[39;00m antigen \u001b[39min\u001b[39;00m train:\n\u001b[1;32m     53\u001b[0m         \u001b[39m# 1.1 Determine its affinity to network cells\u001b[39;00m\n\u001b[0;32m---> 54\u001b[0m         population_affinity \u001b[39m=\u001b[39m [(cell, clonalg\u001b[39m.\u001b[39;49maffinity(cell, antigen)) \u001b[39mfor\u001b[39;49;00m cell \u001b[39min\u001b[39;49;00m population]\n\u001b[1;32m     55\u001b[0m         \u001b[39m# 1.2 Select the n highest affinity network cells\u001b[39;00m\n\u001b[1;32m     56\u001b[0m         population_affinity \u001b[39m=\u001b[39m \u001b[39msorted\u001b[39m(population_affinity, key\u001b[39m=\u001b[39m\u001b[39mlambda\u001b[39;00m x: \u001b[39mabs\u001b[39m(x[\u001b[39m1\u001b[39m]))\n",
      "Cell \u001b[0;32mIn[31], line 54\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[39mwhile\u001b[39;00m stop \u001b[39m!=\u001b[39m stop_condition:\n\u001b[1;32m     51\u001b[0m     \u001b[39m# 1. For each antigen do\u001b[39;00m\n\u001b[1;32m     52\u001b[0m     \u001b[39mfor\u001b[39;00m antigen \u001b[39min\u001b[39;00m train:\n\u001b[1;32m     53\u001b[0m         \u001b[39m# 1.1 Determine its affinity to network cells\u001b[39;00m\n\u001b[0;32m---> 54\u001b[0m         population_affinity \u001b[39m=\u001b[39m [(cell, clonalg\u001b[39m.\u001b[39;49maffinity(cell, antigen)) \u001b[39mfor\u001b[39;00m cell \u001b[39min\u001b[39;00m population]\n\u001b[1;32m     55\u001b[0m         \u001b[39m# 1.2 Select the n highest affinity network cells\u001b[39;00m\n\u001b[1;32m     56\u001b[0m         population_affinity \u001b[39m=\u001b[39m \u001b[39msorted\u001b[39m(population_affinity, key\u001b[39m=\u001b[39m\u001b[39mlambda\u001b[39;00m x: \u001b[39mabs\u001b[39m(x[\u001b[39m1\u001b[39m]))\n",
      "File \u001b[0;32m~/Projetos/Tr-iaNET/clonalg.py:10\u001b[0m, in \u001b[0;36maffinity\u001b[0;34m(cell, antigen)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39maffinity\u001b[39m(cell, antigen):\n\u001b[0;32m---> 10\u001b[0m     diff \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49msubtract(cell, antigen)\n\u001b[1;32m     11\u001b[0m     squared_diff \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39msquare(diff)\n\u001b[1;32m     12\u001b[0m     norm \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39msqrt(np\u001b[39m.\u001b[39msum(squared_diff))\n",
      "\u001b[0;31mValueError\u001b[0m: setting an array element with a sequence. The requested array would exceed the maximum number of dimension of 32."
     ]
    }
   ],
   "source": [
    "import clonalg\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "\n",
    "# Criar um conjunto de dados com 3 clusters bem separados\n",
    "X, y = make_blobs(n_samples=300, centers=3, random_state=42)\n",
    "\n",
    "# Converter os arrays em um DataFrame\n",
    "df = pd.DataFrame(np.column_stack((X, y)))\n",
    "\n",
    "# Dividir o conjunto de dados em treino e teste\n",
    "train_data, test_data = train_test_split(df, test_size=0.3)\n",
    "\n",
    "train_data = train_data.to_numpy()\n",
    "\n",
    "labels = df[[2]].copy()\n",
    "\n",
    "df = df.drop(columns=[2])\n",
    "\n",
    "#Standard Scaler\n",
    "scaler = preprocessing.MinMaxScaler()\n",
    "train = pd.DataFrame(scaler.fit_transform(df.values), columns=df.columns, index=df.index)\n",
    "\n",
    "#Train dataset\n",
    "train = train.join(labels)\n",
    "# O valor de train[4] define qual label será utilizada para o treino da população\n",
    "train = train.loc[train[2] == 2]\n",
    "train = train.drop(columns=[2])\n",
    "\n",
    "population_size = 300\n",
    "selection_size = 80\n",
    "memory_set_percentage = 20\n",
    "clone_rate = 20\n",
    "mutation_rate = 0.3\n",
    "stop_condition = 50\n",
    "d = 15\n",
    "sigma1 = 0.97\n",
    "sigma2 = 0.5\n",
    "feature_num = train_data.shape[1] - 1\n",
    "feature_min = train_data.min().min()\n",
    "feature_max = train_data.max().max()\n",
    "\n",
    "stop = 0\n",
    "population = clonalg.create_random_cells(population_size, feature_num, feature_min, feature_max)\n",
    "\n",
    "\n",
    "while stop != stop_condition:\n",
    "    # 1. For each antigen do\n",
    "    for index, antigen_row in train.iterrows():\n",
    "        antigen = antigen_row.to_numpy()\n",
    "        # 1.1 Determine its affinity to network cells\n",
    "        population_affinity = [(cell, clonalg.affinity(cell, antigen)) for cell in population]\n",
    "        # 1.2 Select the n highest affinity network cells\n",
    "        population_affinity = sorted(population_affinity, key=lambda x: abs(x[1]))\n",
    "        best_affinity = population_affinity[:selection_size]\n",
    "        # 1.3 Generate Nc clones from these n cells. The higher the affinity, the larger Nc;\n",
    "        clone_population = []\n",
    "        for cell in best_affinity:\n",
    "            cell_clones = clonalg.clone(cell, clone_rate)\n",
    "            clone_population.extend(cell_clones)  # Use extend aqui\n",
    "        # 1.4 Apply hypermutation to the generated clones, with variability inversely proportional to the progenitor fitness \n",
    "        # 1.5 Determine the affinity among the antigen and all clones\n",
    "        mutaded_clone_population = []\n",
    "        for cell in clone_population:\n",
    "            mutated_clone = clonalg.hypermutate_variability(cell, mutation_rate, antigen)\n",
    "            mutaded_clone_population.append(mutated_clone)\n",
    "        # 1.6 Keep only m% of the highest affinity mutated clones into the clone population\n",
    "        mutaded_clone_population.sort(key=lambda x: x[1])\n",
    "        pop_size = round(len(clone_population)/100)*memory_set_percentage\n",
    "\n",
    "        mutaded_clone_population = mutaded_clone_population[:pop_size]\n",
    "\n",
    "        # 1.7 Eliminate all clones but one whose affinity with the antigen is inferior to a predefined threshold sigma2 (apoptosis)\n",
    "        filtered_clone_population = list(filter(lambda x: x[1] > sigma2, mutaded_clone_population))\n",
    "\n",
    "        # 1.8 Determine the affinity among all the mutated clonesand eliminate those whose affinity with each other is above a pre-defined threshold sigma1 (supression)\n",
    "\n",
    "        remaining_clone_population = clonalg.remove_similar_clones(filtered_clone_population, sigma1)\n",
    "\n",
    "        # 1.9 Insert the remaining clones into the populatuon\n",
    "        # Remove the affinity attribute from the cells in remaining_clone_population\n",
    "        remaining_clone_population_no_affinity = [(cell[0],) for cell in remaining_clone_population]\n",
    "        # Add remaining_clone_population_no_affinity to the population\n",
    "        population.extend(remaining_clone_population_no_affinity)  # Use extend aqui\n",
    "\n",
    "\n",
    "#     # 2.0 Determine the simillarity among all the antibodies and eliminate those with similarity above a threshold sigma1 (supression)\n",
    "    population = clonalg.remove_similar_clones(population, sigma1)\n",
    "\n",
    "#   3 Introduce a d% of new randomly generated cells (random insertion)\n",
    "    new_cells = clonalg.create_random_cells(int(population_size * (d / 100)), feature_num, feature_min, feature_max)\n",
    "    population.extend(new_cells)  # Use extend aqui\n",
    "\n",
    "    print(\"População: \", len(population), \"     Iteração: \", stop)\n",
    "    stop += 1\n",
    "    \n",
    "## Solução temporária para nested arrays nos indivíduos da população.\n",
    "\n",
    "for i, cell in enumerate(population):\n",
    "    # Verifica se a célula tem mais de uma dimensão\n",
    "    if len(cell[0].shape) > 1:\n",
    "        # Aplica numpy.ravel() para simplificar as dimensões\n",
    "        flattened_cell = np.ravel(cell[0])\n",
    "        population[i] = (flattened_cell,)\n",
    "    else:\n",
    "        population[i] = cell\n",
    "\n",
    "    if isinstance(cell, tuple):\n",
    "        array_cell = np.array(cell)\n",
    "        flattened_cell = np.ravel(array_cell)\n",
    "        population[i] = (flattened_cell)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "antibody_coordinates = [cell for cell in population]\n",
    "x_coordinates = [coord[0] for coord in antibody_coordinates]\n",
    "y_coordinates = [coord[1] for coord in antibody_coordinates]\n",
    "plt.scatter(x_coordinates, y_coordinates, label=f\"Population {i}\")\n",
    "\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.title('Antibody Populations')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(mutaded_clone_population)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(filtered_clone_population)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(remaining_clone_population)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "c3503f95e0e8f4afdf6702396a7a2a29cae9f67572acfe092405dcaa2579b817"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
